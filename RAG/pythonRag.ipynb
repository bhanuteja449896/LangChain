{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183281eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"]=os.getenv(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3968f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "# % pip install pythonloader\n",
    "loader = PythonLoader(\"app.py\")\n",
    "pythondocs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065f5fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'app.py'}, page_content='from fastapi import FastAPI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain_ollama import ChatOllama\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langchain_community.llms import Ollama\\nfrom langserve import add_routes\\nimport uvicorn\\nimport os\\nfrom dotenv import load_dotenv\\n\\n# Load environment variables\\nload_dotenv()\\n\\napp = FastAPI(\\n    title=\"Simple Chatbot API\",\\n    description=\"A simple API for interacting with the TinyLlama chatbot using LangChain and Ollama.\",\\n    version=\"1.0.0\"\\n)\\n\\n\\n\\n\\nllm1 = ChatOllama(model=\"tinyllama\")\\ngemini = ChatGoogleGenerativeAI(\\n    model=\"models/gemini-1.5-flash\",\\n    google_api_key=os.getenv(\"GEMINI_API_KEY\")\\n)\\n\\n\\n\\nprompt1 = ChatPromptTemplate.from_template(\\n    \"You are a helpful assitant prompt : {topic}.\"\\n)\\nprompt2 = ChatPromptTemplate.from_template(\\n    \"You are a helpful assistant prompt : {topic}.\"\\n)\\n\\n\\n\\nadd_routes(\\n    app,\\n    prompt1 | llm1,\\n    path=\"/ollama\"\\n)'),\n",
       " Document(metadata={'source': 'app.py'}, page_content='add_routes(\\n    app,\\n    prompt1 | llm1,\\n    path=\"/ollama\"\\n)\\n\\nadd_routes(\\n    app,\\n    prompt2 | gemini,\\n    path=\"/gemini\"\\n)\\n\\n\\n\\nif __name__ == \"__main__\":\\n    uvicorn.run(app,host=\"localhost\", port=8000, log_level=\"info\")')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(pythondocs)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34833b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000207FECFB250>, search_kwargs={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "df = FAISS.from_documents(\n",
    "    documents[:20], \n",
    "    GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\",\n",
    "        google_api_key=os.environ[\"GEMINI_API_KEY\"]\n",
    "    )\n",
    ")\n",
    "retriever = df.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bea67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents, GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=os.environ[\"GEMINI_API_KEY\"]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d45375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing code for errors...\n",
      "==================================================\n",
      "## ERRORS FOUND:\n",
      "\n",
      "1. **Duplicate Route:** The code registers the `/ollama` route twice using `add_routes`. This will lead to a conflict and only one of the routes will be functional, potentially leading to unexpected behavior.\n",
      "\n",
      "2. **Inconsistent Prompt Templates:** While both `prompt1` and `prompt2` are almost identical (only a minor difference in \"assitant\" vs \"assistant\"), using two separate prompts is redundant.  It's better to use a single prompt template.\n",
      "\n",
      "3. **Missing Error Handling:** The code lacks error handling.  If `os.getenv(\"GEMINI_API_KEY\")` returns `None` (meaning the environment variable isn't set),  `ChatGoogleGenerativeAI` will raise an exception, crashing the application.  Similarly, network issues with Ollama or Google Gemini could also cause crashes.\n",
      "\n",
      "4. **Potential for Large Response Sizes:**  The code doesn't handle potential large responses from the LLMs which could lead to memory issues or slowdowns.  Streaming responses would be a better approach for production-ready code.  However this is beyond the scope of this fix.\n",
      "\n",
      "5. **Missing Type Hints:**  Lack of type hints reduces code readability and maintainability and makes it harder to catch errors early.\n",
      "\n",
      "\n",
      "## CORRECTED CODE:\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "from langchain.prompts import ChatPromptTemplate\n",
      "from langchain_ollama import ChatOllama\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "from langserve import add_routes\n",
      "import uvicorn\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "# Load environment variables\n",
      "load_dotenv()\n",
      "\n",
      "app = FastAPI(\n",
      "    title=\"Simple Chatbot API\",\n",
      "    description=\"A simple API for interacting with chatbots using LangChain.\",\n",
      "    version=\"1.0.0\"\n",
      ")\n",
      "\n",
      "#Added type hints for better readability and maintainability.\n",
      "def get_llm(model_name: str, api_key: Optional[str] = None) -> ChatGoogleGenerativeAI | ChatOllama:\n",
      "    if model_name == \"tinyllama\":\n",
      "        return ChatOllama(model=model_name)\n",
      "    elif model_name == \"gemini\":\n",
      "        if api_key is None:\n",
      "            raise ValueError(\"GEMINI_API_KEY environment variable not set.\")\n",
      "        return ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", google_api_key=api_key)\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
      "\n",
      "#Using a single prompt template\n",
      "prompt = ChatPromptTemplate.from_template(\n",
      "    \"You are a helpful assistant. Prompt: {topic}\"\n",
      ")\n",
      "\n",
      "#Added error handling and model selection\n",
      "try:\n",
      "    ollama_llm = get_llm(\"tinyllama\")\n",
      "    gemini_llm = get_llm(\"gemini\", os.getenv(\"GEMINI_API_KEY\"))\n",
      "\n",
      "    add_routes(app, prompt | ollama_llm, path=\"/ollama\")\n",
      "    add_routes(app, prompt | gemini_llm, path=\"/gemini\")\n",
      "except ValueError as e:\n",
      "    print(f\"Error initializing LLMs: {e}\")\n",
      "    exit(1)  # Exit with an error code\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    uvicorn.run(app, host=\"localhost\", port=8000, log_level=\"info\")\n",
      "\n",
      "```\n",
      "\n",
      "## EXPLANATIONS:\n",
      "\n",
      "1. **Duplicate Route Removal:** The duplicated `/ollama` route was removed.  Only one route now exists for each LLM.\n",
      "\n",
      "2. **Consolidated Prompt:**  `prompt1` and `prompt2` were combined into a single `prompt` variable.\n",
      "\n",
      "3. **Error Handling Added:** A `try...except` block now handles potential `ValueError` exceptions during LLM initialization, preventing the application from crashing if the API key is missing or an invalid model is specified. A `get_llm` function is introduced to encapsulate this logic.\n",
      "\n",
      "4. **Improved Function Structure:**  The code has been refactored to use a function `get_llm` to handle LLM instantiation, improving readability and maintainability.\n",
      "\n",
      "5. **Type Hints Added:** Type hints have been added to improve code clarity and readability.  This makes it easier to understand the data types involved in the code.\n",
      "\n",
      "\n",
      "This corrected code is more robust, readable, and less prone to errors.  Remember to install the necessary packages: `pip install fastapi uvicorn langchain langchain-ollama langchain-google-genai langserve python-dotenv`.  Also ensure you have the `GEMINI_API_KEY` environment variable set correctly.  For Ollama, ensure that the Ollama daemon is running.\n",
      "## ERRORS FOUND:\n",
      "\n",
      "1. **Duplicate Route:** The code registers the `/ollama` route twice using `add_routes`. This will lead to a conflict and only one of the routes will be functional, potentially leading to unexpected behavior.\n",
      "\n",
      "2. **Inconsistent Prompt Templates:** While both `prompt1` and `prompt2` are almost identical (only a minor difference in \"assitant\" vs \"assistant\"), using two separate prompts is redundant.  It's better to use a single prompt template.\n",
      "\n",
      "3. **Missing Error Handling:** The code lacks error handling.  If `os.getenv(\"GEMINI_API_KEY\")` returns `None` (meaning the environment variable isn't set),  `ChatGoogleGenerativeAI` will raise an exception, crashing the application.  Similarly, network issues with Ollama or Google Gemini could also cause crashes.\n",
      "\n",
      "4. **Potential for Large Response Sizes:**  The code doesn't handle potential large responses from the LLMs which could lead to memory issues or slowdowns.  Streaming responses would be a better approach for production-ready code.  However this is beyond the scope of this fix.\n",
      "\n",
      "5. **Missing Type Hints:**  Lack of type hints reduces code readability and maintainability and makes it harder to catch errors early.\n",
      "\n",
      "\n",
      "## CORRECTED CODE:\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "from langchain.prompts import ChatPromptTemplate\n",
      "from langchain_ollama import ChatOllama\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "from langserve import add_routes\n",
      "import uvicorn\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "# Load environment variables\n",
      "load_dotenv()\n",
      "\n",
      "app = FastAPI(\n",
      "    title=\"Simple Chatbot API\",\n",
      "    description=\"A simple API for interacting with chatbots using LangChain.\",\n",
      "    version=\"1.0.0\"\n",
      ")\n",
      "\n",
      "#Added type hints for better readability and maintainability.\n",
      "def get_llm(model_name: str, api_key: Optional[str] = None) -> ChatGoogleGenerativeAI | ChatOllama:\n",
      "    if model_name == \"tinyllama\":\n",
      "        return ChatOllama(model=model_name)\n",
      "    elif model_name == \"gemini\":\n",
      "        if api_key is None:\n",
      "            raise ValueError(\"GEMINI_API_KEY environment variable not set.\")\n",
      "        return ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", google_api_key=api_key)\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
      "\n",
      "#Using a single prompt template\n",
      "prompt = ChatPromptTemplate.from_template(\n",
      "    \"You are a helpful assistant. Prompt: {topic}\"\n",
      ")\n",
      "\n",
      "#Added error handling and model selection\n",
      "try:\n",
      "    ollama_llm = get_llm(\"tinyllama\")\n",
      "    gemini_llm = get_llm(\"gemini\", os.getenv(\"GEMINI_API_KEY\"))\n",
      "\n",
      "    add_routes(app, prompt | ollama_llm, path=\"/ollama\")\n",
      "    add_routes(app, prompt | gemini_llm, path=\"/gemini\")\n",
      "except ValueError as e:\n",
      "    print(f\"Error initializing LLMs: {e}\")\n",
      "    exit(1)  # Exit with an error code\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    uvicorn.run(app, host=\"localhost\", port=8000, log_level=\"info\")\n",
      "\n",
      "```\n",
      "\n",
      "## EXPLANATIONS:\n",
      "\n",
      "1. **Duplicate Route Removal:** The duplicated `/ollama` route was removed.  Only one route now exists for each LLM.\n",
      "\n",
      "2. **Consolidated Prompt:**  `prompt1` and `prompt2` were combined into a single `prompt` variable.\n",
      "\n",
      "3. **Error Handling Added:** A `try...except` block now handles potential `ValueError` exceptions during LLM initialization, preventing the application from crashing if the API key is missing or an invalid model is specified. A `get_llm` function is introduced to encapsulate this logic.\n",
      "\n",
      "4. **Improved Function Structure:**  The code has been refactored to use a function `get_llm` to handle LLM instantiation, improving readability and maintainability.\n",
      "\n",
      "5. **Type Hints Added:** Type hints have been added to improve code clarity and readability.  This makes it easier to understand the data types involved in the code.\n",
      "\n",
      "\n",
      "This corrected code is more robust, readable, and less prone to errors.  Remember to install the necessary packages: `pip install fastapi uvicorn langchain langchain-ollama langchain-google-genai langserve python-dotenv`.  Also ensure you have the `GEMINI_API_KEY` environment variable set correctly.  For Ollama, ensure that the Ollama daemon is running.\n"
     ]
    }
   ],
   "source": [
    "# AI-powered Error Detection and Code Fixing\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-1.5-flash\",\n",
    "    google_api_key=os.environ[\"GEMINI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Create a prompt template for error detection and fixing\n",
    "error_fixing_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert Python code reviewer and debugger. Analyze the following code for errors and issues:\n",
    "\n",
    "CODE TO ANALYZE:\n",
    "{code}\n",
    "\n",
    "Please:\n",
    "1. Identify all errors, bugs, and potential issues\n",
    "2. Explain what each error is and why it's problematic\n",
    "3. Provide the corrected version of the code\n",
    "4. Add comments explaining the fixes\n",
    "\n",
    "Format your response as:\n",
    "## ERRORS FOUND:\n",
    "[List all errors with explanations]\n",
    "\n",
    "## CORRECTED CODE:\n",
    "```python\n",
    "[Provide the fixed code with comments]\n",
    "```\n",
    "\n",
    "## EXPLANATIONS:\n",
    "[Explain the fixes made]\n",
    "\"\"\")\n",
    "\n",
    "# Get the full code content\n",
    "full_code = \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "print(\"üîç Analyzing code for errors...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create the chain and invoke it\n",
    "chain = error_fixing_prompt | llm\n",
    "result = chain.invoke({\"code\": full_code})\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Code Error Checker\n",
    "def check_code_errors():\n",
    "    print(\"üìù Interactive Code Error Checker\")\n",
    "    print(\"Enter your code (press Enter twice to finish):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    lines = []\n",
    "    while True:\n",
    "        line = input()\n",
    "        if line == \"\" and len(lines) > 0 and lines[-1] == \"\":\n",
    "            break\n",
    "        lines.append(line)\n",
    "    \n",
    "    # Remove the last empty line\n",
    "    if lines and lines[-1] == \"\":\n",
    "        lines.pop()\n",
    "    \n",
    "    user_code = \"\\n\".join(lines)\n",
    "    \n",
    "    if not user_code.strip():\n",
    "        print(\"‚ùå No code provided!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüîç Analyzing your code...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Use the same error-fixing chain\n",
    "    result = chain.invoke({\"code\": user_code})\n",
    "    print(result.content)\n",
    "\n",
    "# Call the interactive checker\n",
    "check_code_errors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
